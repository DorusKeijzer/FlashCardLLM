{
from llama_cpp import Llama

llm = Llama.from_pretrained(
      # seed=1337, # Uncomment to set a specific seed
      # n_ctx=2048, # Uncomment to increase the context window
      repo_id="Qwen/Qwen2-0.5B-Instruct-GGUF",
      n_gpu_layers=-1, # Uncomment to use GPU acceleration
      filename="*q8_0.gguf",
      verbose=False
)
output = llm.create_chat_completion(
      messages = [
          {"role": "system", "content": "You are an assistant who perfectly describes images."},
          {
              "role": "user",
              "content": "Write a limerick about machine learning"
          }
      ]
)


print(output)
   "349": "3030",
   "aefin": "j"
}
